{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44a9ac0-58fd-4a9b-b7d3-7f5589be6be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold-1\n",
      "Fold-1 AUC: 0.9970, Kappa: 0.9924\n",
      "Running fold-2\n",
      "Fold-2 AUC: 0.9977, Kappa: 0.9922\n",
      "Running fold-3\n",
      "Fold-3 AUC: 0.9995, Kappa: 0.9931\n",
      "Running fold-4\n",
      "Fold-4 AUC: 0.9985, Kappa: 0.9918\n",
      "Running fold-5\n",
      "Fold-5 AUC: 0.9993, Kappa: 0.9912\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average AUC: 0.9984\n",
      "Average Kappa: 0.9921\n",
      "Average Class-Based Accuracy:\n",
      "Class 0: 0.9981\n",
      "Class 1: 0.8746\n",
      "Class 2: 0.9883\n",
      "Class 3: 0.7397\n",
      "Class 4: 0.9969\n",
      "K-Fold cross-validation completed. Results saved in /Users/himanshupradhan/coding/Projects/Major Project/nid-system/notebooks/Himanshu_KDD+ Dataset-multiclass/xgboost\n",
      "Training final XGBoost model...\n",
      "\n",
      "Test Set Evaluation:\n",
      "AUC: 0.8705\n",
      "Kappa: 0.5472\n",
      "Class-Based Accuracy:\n",
      "Class 0: 0.7937\n",
      "Class 1: 0.0056\n",
      "Class 2: 0.7007\n",
      "Class 3: 0.0019\n",
      "Class 4: 0.9392\n",
      "Final model evaluation completed. Results saved in /Users/himanshupradhan/coding/Projects/Major Project/nid-system/notebooks/Himanshu_KDD+ Dataset-multiclass/xgboost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, cohen_kappa_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from utils.preprocess_data_KDD_plus_test import preprocess_data\n",
    "from utils.plot import plot_combined_roc_curve\n",
    "\n",
    "# File paths\n",
    "train_file_path = os.path.join('data', 'KDDTrain+.csv')\n",
    "test_file_path = os.path.join('data', 'KDDTest+.csv')\n",
    "\n",
    "# Preprocess data\n",
    "preprocessing_models_folder = os.path.join('preprocessing_pipeline')\n",
    "X_train, X_test, y_train, y_test = preprocess_data(\n",
    "    train_file_path=train_file_path,\n",
    "    test_file_path=test_file_path,\n",
    "    preprocessing_models_folder=preprocessing_models_folder\n",
    ")\n",
    "\n",
    "# Set output folder\n",
    "current_directory = os.path.abspath(os.getcwd())\n",
    "output_folder = os.path.join(current_directory, \"xgboost\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def class_based_accuracy(y_true, y_pred, num_classes):\n",
    "    \"\"\"Calculate class-based accuracy for each class.\"\"\"\n",
    "    class_acc = {}\n",
    "    for cls in range(num_classes):\n",
    "        mask = y_true == cls\n",
    "        acc = accuracy_score(y_true[mask], y_pred[mask])\n",
    "        class_acc[cls] = acc\n",
    "    return class_acc\n",
    "\n",
    "\n",
    "def xgboost_kfold(X, y, num_classes, k=5, output_folder=output_folder):\n",
    "    \"\"\"Train XGBoost with K-Fold cross-validation and evaluate with metrics.\"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    all_auc = []\n",
    "    all_kappa = []\n",
    "    class_accuracies = []\n",
    "\n",
    "    for fold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Running fold-{fold_index + 1}\")\n",
    "        X_train_fold, X_val_fold = X[train_index], X[test_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[test_index]\n",
    "\n",
    "        # Initialize and train the XGBoost model\n",
    "        xgb = XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=num_classes,\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=42\n",
    "        )\n",
    "        xgb.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict probabilities and classes\n",
    "        y_prob_val = xgb.predict_proba(X_val_fold)\n",
    "        y_pred_val = np.argmax(y_prob_val, axis=1)\n",
    "\n",
    "        # Compute metrics for the fold\n",
    "        fold_auc = roc_auc_score(y_val_fold, y_prob_val, multi_class='ovr')\n",
    "        fold_kappa = cohen_kappa_score(y_val_fold, y_pred_val)\n",
    "        fold_class_acc = class_based_accuracy(y_val_fold, y_pred_val, num_classes)\n",
    "\n",
    "        all_auc.append(fold_auc)\n",
    "        all_kappa.append(fold_kappa)\n",
    "        class_accuracies.append(fold_class_acc)\n",
    "\n",
    "        print(f\"Fold-{fold_index + 1} AUC: {fold_auc:.4f}, Kappa: {fold_kappa:.4f}\")\n",
    "\n",
    "    # Combine metrics across folds\n",
    "    avg_auc = np.mean(all_auc)\n",
    "    avg_kappa = np.mean(all_kappa)\n",
    "    avg_class_acc = {cls: np.mean([acc[cls] for acc in class_accuracies]) for cls in range(num_classes)}\n",
    "\n",
    "    print(\"\\nK-Fold Cross-Validation Results:\")\n",
    "    print(f\"Average AUC: {avg_auc:.4f}\")\n",
    "    print(f\"Average Kappa: {avg_kappa:.4f}\")\n",
    "    print(\"Average Class-Based Accuracy:\")\n",
    "    for cls, acc in avg_class_acc.items():\n",
    "        print(f\"Class {cls}: {acc:.4f}\")\n",
    "\n",
    "    print(f\"K-Fold cross-validation completed. Results saved in {output_folder}\")\n",
    "\n",
    "\n",
    "def train_xgboost(X, y, X_test, y_test, num_classes, output_folder=output_folder):\n",
    "    \"\"\"Train the final XGBoost model on the full dataset and evaluate on the test set.\"\"\"\n",
    "    print(\"Training final XGBoost model...\")\n",
    "    xgb_final = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=num_classes,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_final.fit(X, y)\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(output_folder, \"xgboost_model.pkl\")\n",
    "    joblib.dump(xgb_final, model_path)\n",
    "\n",
    "    # Predict probabilities and classes for the test set\n",
    "    y_prob_test = xgb_final.predict_proba(X_test)\n",
    "    y_pred_test = np.argmax(y_prob_test, axis=1)\n",
    "\n",
    "    # Compute metrics\n",
    "    test_auc = roc_auc_score(y_test, y_prob_test, multi_class='ovr')\n",
    "    test_kappa = cohen_kappa_score(y_test, y_pred_test)\n",
    "    test_class_acc = class_based_accuracy(y_test, y_pred_test, num_classes)\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Kappa: {test_kappa:.4f}\")\n",
    "    print(\"Class-Based Accuracy:\")\n",
    "    for cls, acc in test_class_acc.items():\n",
    "        print(f\"Class {cls}: {acc:.4f}\")\n",
    "\n",
    "    print(f\"Final model evaluation completed. Results saved in {output_folder}\")\n",
    "\n",
    "\n",
    "# Run K-Fold cross-validation\n",
    "num_classes = len(np.unique(y_train))  # Number of unique classes in the dataset\n",
    "xgboost_kfold(X_train, y_train, num_classes=num_classes, k=5, output_folder=output_folder)\n",
    "\n",
    "# Train the final model and evaluate on the test set\n",
    "train_xgboost(X_train, y_train, X_test, y_test, num_classes=num_classes, output_folder=output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e6052-fc25-495c-9ddc-71caa10ce98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310tf2",
   "language": "python",
   "name": "py310tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
